---
title: LLMs
layout: page
permalink: /methods/llms/
parent: Methods
nav_order: 1
---

## **Fundamentals**
<ul>
<li> <a href="https://www.cs.toronto.edu/~hector/Papers/ijcai-13-paper.pdf"> On Our Best Behaviour - Levesque</a> </li>
<li> <a href="https://arxiv.org/abs/1310.4546"> Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al. (2013) </a> </li>
<li> <a href="https://arxiv.org/abs/1802.05365"> Deep Contextualized Word Representations - Peters et al. (2018)  </a> </li>
<li> <a href="https://arxiv.org/abs/1810.04805"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Devlin et al. (2018)</a></li>
<li> <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"> Improving Language Understanding by Generative Pre-Training - Radford et al. </a></li>
<li> <a href="https://arxiv.org/abs/2005.14165"> Language Models are Few-Shot Learners - Brown et al.</a></li>
</ul>

## **Reinforcement Learning With Human Feedback**
<ul>
<li> <a href="https://arxiv.org/abs/1706.03741"> Deep reinforcement learning from human preferences (Christiano et al.)</a></li>
<li> <a href="https://arxiv.org/abs/1909.08593"> Fine-tuning language models from human preferences (Ziegler et al.)</a> </li>
<li> <a href="https://arxiv.org/abs/2204.05862"> Training a helpful and harmless assistant with
reinforcement learning from human feedback  (Bai et al.)</a> </li>
<li> <a href=""> Open Problems and Fundamental Limitations of
Reinforcement Learning from Human Feedback (Casper et al.) </a> </li>
</ul>

## **Scaling Laws**
<ul>
<li> <a href="https://arxiv.org/abs/2305.16264"> Scaling Data-Constrained Language Models - Muennighoff et al.</a </li>
</ul>

## **Training**
<ul>
<li> <a href="https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/"> Token selection strategies - Chng (2023)</a> </li>
</ul>

